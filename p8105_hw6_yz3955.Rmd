---
title: "HW6 SOLUTION"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(modelr)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)
```


Start with one city.

```{r}

#binomal(link=’logit’) #output follows binomial，link regression is logit，logistic regression
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")
glm(resolution ~ victim_age + victim_race + victim_sex, 
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)

## recall for logistic regression calculation equation:https://stats.stackexchange.com/questions/354098/calculating-confidence-intervals-for-a-logistic-regression
```


Try this across cities.

```{r}
models_results_df = 
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate), #  the odds ratio can be computed by raising e to the power of the logistic coefficient,
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI")) 
```

```{r}
models_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



## Problem 2

### load data

```{r}
baby_df = 
  read_csv("./data/birthweight.csv") %>% 
  mutate(
    babysex = ifelse(babysex == 1, "male", "female"),
    babysex = as.factor(babysex),
    frace = case_when(
      frace == 1 ~ "White", 
      frace == 2 ~ "Black", 
      frace == 3 ~ "Asian",
      frace == 4 ~ "Puerto Rican",
      frace == 8 ~ "Other",
      frace == 9 ~ "Unknown"),
    mrace = case_when(
      mrace == 1 ~ "White", 
      mrace == 2 ~ "Black", 
      mrace == 3 ~ "Asian",
      mrace == 4 ~ "Puerto Rican",
      mrace == 8 ~ "Other"),
   malform = ifelse(malform == 0, "absert", "present"),
   malform = as.factor(malform)
  )
```

### fit a model:
In order to get a hypothesized structure of linear model, I used Stepwise regression with both direction.  


First build the hypothesized model:
```{r warning = FALSE}
model_fit = 
  lm(bwt ~., data = baby_df) %>% 
  step(direction = "both")
summary(model_fit)
```
After calculting AIC score for adding a variable or remove a variable, the final model for birthweight prediction includes variables:babysexmale, bhead, blength, delwt, fincome, gaweeks, mheight, mrace, parity, ppwt, smoken.  


#### a plot of model residuals against fitted values:
```{r}
baby_df %>% 
  modelr::add_residuals(model_fit) %>% 
  add_predictions(model_fit) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point()
```
The points of residuals vs fitted plot concentrated at residuals = 0.  
The residuals roughly form a "horizontal band" around the 0 line but the residuals doesn't seem to "bounce randomly" around the 0 line.

### create two other models
fit2 using length at brith and gestational age as predictors.  
fit3 using head circumference, length, sex, and all interactions (including the three-way interaction) between these
```{r warning=FALSE}
fit2 = lm(bwt ~ blength + gaweeks, data = baby_df)
fit3 = lm(bwt ~ blength * bhead * babysex, data = baby_df)
```

### cross validation

```{r warning=FALSE}
birthw_cv = 
  crossv_mc(baby_df, 1000) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble),
    step_mod = map(.x = train, ~lm(bwt ~., data = .x)),
    fit2_mod = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    fit3_mod = map(.x = train, ~lm(bwt ~ blength * bhead * babysex, data = .x))) %>% 
  mutate(
    rmse_step = map2_dbl(step_mod, test, ~rmse(model = .x, data = .y)),
    rmse_fit2    = map2_dbl(fit2_mod, test, ~rmse(model = .x, data = .y)),
    rmse_fit3 = map2_dbl(fit3_mod, test, ~rmse(model = .x, data = .y))
  )

```

#### make a boxplot of rmse in different models
```{r}
birthw_cv %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```

From above boxplot, we can easily see that my model(stepwise regression) is the optimal one. It has the smallest mean and median of rmse. Fit3 is better than fit1, which means that the model should not only include length at birth and gestational age as predictors.  


## Problem 3

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

look tmax vs tmin scatter plot
```{r}
weather_df %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_point() +
  geom_smooth(method = "lm")
```
It seems like a simple linear regression

### 5000 Bootstrap samples

```{r}
set.seed(1)
weather_bootst = 
  weather_df %>% 
  modelr::bootstrap(n = 5000, id = "id") %>% 
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)),
    rsq_results =  map(models, broom::glance),
    tidy_result = map(models, broom::tidy)
    ) %>% 
  select(id, rsq_results,tidy_result) %>% 
  unnest(rsq_results) %>% 
  subset(select = c(id, r.squared, tidy_result)) 

```

#### Plot the distribution of r^2
```{r}
weather_bootst %>% 
  ggplot(aes(x = r.squared)) +
  geom_density()
```

#### 95% confidence interval for r^2
```{r}
weather_bootst %>% 
  summarize(
    ci_lower = quantile(r.squared, 0.025), 
    ci_upper = quantile(r.squared, 0.975)) %>% 
  knitr::kable(digits = 3)
```

#### Plot the distribution of log(β̂ 0∗β̂ 1)

```{r}
weather_bootst = 
  weather_bootst %>% 
  unnest(tidy_result) %>%
  select(id, term, estimate) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  rename(intercept = `(Intercept)`) %>% 
  mutate(
    logb0_b1 = log(intercept*tmin)
  ) 
  
weather_bootst %>% 
  ggplot(aes(x = logb0_b1)) +
  geom_density() +
  xlab("log(β 0 * β̂ 1)")
```

#### 95% confidence interval for log(β̂ 0∗β̂ 1)

```{r}
weather_bootst %>% 
  summarize(
    ci_lower = quantile(logb0_b1, 0.025), 
    ci_upper = quantile(logb0_b1, 0.975)) %>% 
  knitr::kable(digits = 3)
```

